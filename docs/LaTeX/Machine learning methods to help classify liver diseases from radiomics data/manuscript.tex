\documentclass[10pt]{article}
\newcommand\hmmax{0}
\newcommand\bmmax{0}
\usepackage[T1]{fontenc} 
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{mathrsfs}
\usepackage{anyfontsize}
\usepackage{mdframed}
\usepackage{tikz}
\usepackage{verbatim}

\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\rhead{L2S}
\lhead{Report on work done during first two months of internship }
\rfoot{Page \thepage}

\usepackage{blindtext}

\usepackage{MnSymbol}

\usepackage[cal=boondox,scr=boondoxo]{mathalfa}

\usepackage{float}
\usepackage{subfig}
\usepackage{fancybox,graphicx}
\usepackage{subfig}
\usepackage{caption}

%\usepackage{subcaption}
\usepackage{color}
\usepackage{authblk}
\usepackage{amsmath}  
\usepackage{stmaryrd}  
\usepackage{bm}  
\usepackage{bbm}

%\usepackage[colorlinks]{hyperref}
\usepackage{hyperref} % pour les liens hypertextes
\usepackage{cleveref}
% Configuration de hyperref pour des liens en noir
\hypersetup{
    colorlinks=true,
    linkcolor=black,
    citecolor=black,
    filecolor=black,
    urlcolor=black
}

\input{doiCmd} %doi command

\usepackage{accents}
\usepackage[titletoc,title]{appendix}
%\usepackage[numbers,sort&compress]{natbib}
\usepackage{cite}


%floor and ceiling functions
\usepackage{mathtools}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}



\usepackage[top=2in, bottom=1.5in, left=1in, right=1in]{geometry}
\newcommand{\soft}{\mathcal{S}}
\newcommand{\hard}{\mathcal{H}}
\newcommand*\underdot[1]{ \underaccent{\bullet}{\mathcal{#1}} } %requiere: \usepackage{accents} 
\newcommand*\UnderDot[1]{ \underaccent{\bullet}{#1} } %requiere: \usepackage{accents} 

\usepackage{stackengine}
\newcommand\barbelow[1]{\stackunder[2.5pt]{$#1$}{\rule{1.2ex}{.15ex}}}

\newcommand{\pvec}[1]{\vec{#1}\mkern2mu\vphantom{#1}} %to prime a vector

\newcommand*\UnderTilde[1]{ \underaccent{\sim}{#1} }
  
\renewcommand{\figurename}{Fig.}


%definitions
\def\components{
\left(
\begin{matrix}
    z\cos\gamma_n \vspace{0.3cm} \\
    z\sin\gamma_n\vspace{0.3cm} \\
    -(x-x_n)\cos\gamma_n+(y-y_n)\sin\gamma_n
\end{matrix}
\right)
}

\def\componentsII{
\left(
\begin{matrix}
    z\cos\gamma_n \vspace{0.3cm} \\
    z\sin\gamma_n\vspace{0.3cm} \\
    \mathscr{R}_n\cos(\gamma_n+\beta_n)-x\cos\gamma_n+y\sin\gamma_n
\end{matrix}
\right)
}

\def\change{
\scriptsize
\begin{matrix}
    
    y \rightarrow \Lambda_{n}(\boldsymbol{r})
    \vspace{0.05cm} \\
    r \rightarrow \lambda_{n}(\boldsymbol{r})
\end{matrix}
\normalsize
}



\title{Machine learning methods to help classify liver tumors from radiomics data} 
 
\author[1]{Alexandre SELVESTREL}


\affil[1]{Laboratoire des syst\`emes, Orsay France}
\affil[2]{Centrale-Sup\'elec, Gif-sur-Yvette France}
%\affil[3]{Grupo de F\'isica Te\'orica y desrrollo de Software, Universidad Distrital Francisco Jos\'e de Caldas, Bogot\'a, Colombia}
%\affil[4]{University of Vienna, UniWien, Computational physics group}

\begin{document}

    \date{}
    \maketitle

\begin{abstract}
\noindent In this report, we investigate whether tensor-based machine learning algorithms outperform more traditional machine learning on tensor-structured data. Specifically, we benchmark these methods on a radiomic and clinical dataset of liver tumors. The radiomic variables for each tumor are categorized into three main groups: first-order, shape, and texture, and are observed at four time points corresponding to different phases after contrast medium injection: arterial, portal, venous, and late. This organization gives the radiomic data a tensorial structure, and we aim to determine if models that leverage this structure lead to better predictions.\\
\indent More precisely, we aim to distinguish patients whose tumor type is "CCK" from those whose tumor type is "CHC".  This is a complex task, since only microscopy of cancerous tissue can characterize without hesitation the tumor type. Indeed, experts do not always agree with each other when they analyze X-rays alone. We begin by proposing a logistic regression penalized by lasso as a baseline. We then compare its results with those of algorithms exploiting the tensorial structure of the data: group lasso \cite{grp_lasso}, multiway logistic regression \cite{multi_rank_1,multi_rank_r} and finally multiway and multiblock logistic regression.
\\\\Keywords: Machine learning, Multibloc, Multiway, Liver tumor .  
\end{abstract}


\section{Introduction}

We speak of tensorial data as soon as the explanatory variables are structured along several dimensions. In our case, we have two dimensions for our explanatory variables: the quantity of interest and the time at which it is measured (arterial, portal, venous or late). So, instead of having a matrix of explanatory variables, we have a tensor of explanatory variables $\underline{\mathbf{X}} = (x_{ijk})_{i \in \llbracket 1, n \rrbracket, j \in \llbracket 1, J \rrbracket, k \in \llbracket 1, K \rrbracket}$, where $i$ is the individual, $j$ is the quantity of interest and $k$ is the observation time. To avoid confusion with the notion of dimension of a vector space, and in line with the conventions promoted in \textit{Kolda and Bader} (2009) \cite{conventions}, the three dimensions of the tensor $\underline{\mathbf{X}}$ will be referred to as "mode" in the following.\\
\indent In order to train models that are not specific to tensor data (like a penalized logistic regression), we unfold the tensor $\underline{\mathbf{X}}$ and transform it into a matrix by placing each slice at a fixed $k$ next to the other. This gives the following matrix: $\mathbf{X}_{(1)} = [\mathbf{X}_{:\,:1}, \ldots, \mathbf{X}_{:\,:K}]$. However, in doing so, the model is no longer informed that $\underline{\mathbf{X}}$ is made up of the same quantities observed at different times. One of the reason for which this is important is that there is a non negligeable correlation between variables describing the same quantity of interest at different times. We'll still use the group lasso to restore this information to the model by grouping together variables belonging to the same time, but this is not enough to recover the full tensor aspect of the initial data. Furthermore, as pointed out by \textit{Le Brusquet et al.} (2014) \cite{multi_rank_1}, unfolding the matrix greatly increases the number of predictors compared to tensor methods, which can increase computation times. However, given that in our dataset, $K$ is small ($K = 4$), this increase in computation time is not observed with our data.\\

Introduire ici les CHC et CCK et l'interet de l'etude. Les mixtes notamment.

\section{Notations}

Nous précisons dans cette section les notations spécifiques utilisées dans ce papier. \\[5 pt]
\noindent $\bullet \; \;$ La concaténation de deux matrices $\mathbf{A}$ et $\mathbf{B}$ en juxtaposant leurs colonnes côte à côte est notée $[\mathbf{A}  \; \; \mathbf{B}]$.\\
$\bullet \; \;$ Afin d'éviter de trop utiliser le symbole $\phantom{a}^T$, on définit aussi une notation pour désigner la juxtaposition de deux matrices l'une en dessous de l'autre. Ainsi, la matrice définie par bloc avec $\mathbf{A}$ au-dessus de $\mathbf{B}$ est notée $\left[\mathbf{A}; \, \mathbf{B}\right]$. On peut aussi l'écrire $[\mathbf{A}^T \; \; \; \mathbf{B}^T]^T$ mais cela multiplie les symboles $\phantom{a}^T$, ce qui nuit à la lisibilité. \\
$\bullet \; \;$ Les vecteurs étant des matrices colonnes, avec les mêmes notations, on écrira la concaténation de deux vecteurs $\mathbf{u}$ et $\mathbf{v}$ de la manière suivante: $[\mathbf{u} \; \downarrow \;\mathbf{v}]$.  \\
$\bullet \; \;$ Le vecteur (colonne) dont les éléments sont $(u_i)_{i \in \llbracket 1, I\rrbracket}$ est aussi noté $(u_1, u_2, \; \; \hdots \;\; u_I)$. \\
$\bullet \; \;$ On note $\text{Diag}(\mathbf{u})$ la matrice diagonale dont la diagonale est le vecteur $\mathbf{u}$ \\
$\bullet \; \;$ Le vecteur de longueur $I$ rempli de $1$ est noté $\mathbbm{1}_I$.\\
$\bullet \; \;$ Pour les autres notations, nous utilisons les conventions employées par \textit{Kolda and Bader} (2009) \cite{conventions}.

\section{State of the art}
\blindtext

\section{Detailed presentation of the data}
Détailler le jeu de données et expliquer qu'on a enlevé la phase veineuse. Indiquer aussi qu'on a des données cliniques en plus... Justifier le refus de l'inputation de données (qui risquerait de biaiser les résultats vu le nombre de données manquantes sur certains individus)


\section{Methodology}
\noindent In this section, we present in detail the different models that we benchmarked on our dataset. We describe briefly the non tensorial methods so we can focus more on the tensorial ones.
\subsection{Non tensorial methods}
For these methods, we start by unfolding the tensorial data $\underline{\mathbf{X}}$ into the matrix $\mathbf{X}_{(1)} = [\mathbf{X}_{:\,:1}, \ldots \mathbf{X}_{:\,:K}]$. We then complete this matrix  by concatenating (along the columns) the matrix of non tensorial data $\mathbf{X}_{\text{tab}}$ (where "tab" stands for "tabular"). By doing so we obtain $\mathbf{X}_{\text{tot}} = [\mathbf{X}_{(1)}  \; , \; \mathbf{X}_{\text{tab}}]$.\\
\indent We first train a penalized logistic regression lasso on $\mathbf{X}_{\text{tot}}$. Then, still based on the matrix $\mathbf{X}_{\text{tot}}$, we train a group lasso \cite{grp_lasso}. The idea of this second model is to group together all the explanatory variables corresponding to the same observation time, in order to give the model a vague description of the tensorial structure of the data. We'll also compare the results with the case where we group the explanatory variables according to their type: first order, shape and texture, and with the case where we group the variables according to the quantity of interest they describe. In all cases, we also group the clinical variables together.

\subsection{Multiway logistic regression with lasso}
\indent We now turn to tensor approaches. We start by studying a multiway logistic regression penalized by the lasso. This model is described for rank 1 in \textit{Le Brusquet et al.} (2014) \cite{multi_rank_1} and in \textit{Girka et al.} (2021) \cite{multi_rank_r} for its extension to rank $R \in \mathbb{N}^{*}$. In this report, we directly describe the generalization to rank $R \in \mathbb{N}^{*}$, rank $1$ being a special case of this model.\\[5 pt]

\noindent The fundamental idea of the model is to decompose the parameter $\bm{\beta}_{\text{tens}} \in \mathbb{R}^{JK}$ associated with the tensor explanatory variables of the logistic regression as:
\begin{equation}
\bm{\beta}_{\text{tens}} = \sum\limits_{r = 1}^R\bm{\beta}_r^K \otimes \bm{\beta}_r^J
\end{equation}
with for all $r \in \llbracket 1 ,R \rrbracket$, $\bm{\beta}_r^J \in \mathbb{R}^J$ and $\bm{\beta}_r^K \in \mathbb{R}^K$. To take account of the $M$ clinical variables, which are not tensorial, we associate them with a coefficient $\bm{\beta}_{\text{uni}} \in \mathbb{R}^M$. In this way, the parameter $\bm{\beta}$ of the logistic regression is written: $\left[(\bm{\beta_{\text{tens}}})^T \; \; \; (\bm{\beta_{\text{uni}}})^T \right]^T$. 
We then use the logit link function to obtain the following liklyhood for the explained Bernoulli variable $y_i$ ($i \in \llbracket 1, n \rrbracket$):... Parler plutôt de proba condit...
\begin{equation}
\label{eqref:vraisemblance}
\mathfrak{L}( y_i \, | \, \mathbf{x}_i) = \frac{1}{1 + \exp(- \mathbf{x}_i^T \bm{\beta} - \beta_0)}
\end{equation}
where  $\beta_0 \in \mathbb{R}$ is the intercept and $\mathbf{x}_i \in \mathbb{R}^{JK}$ is given by $\mathbf{x}_i = {(\mathbf{X}_{\text{tot}}})_{i:}$ \\

\noindent We set  $\bm{\beta}^J = \left[\left(\bm{\beta}_1^J\right)^T \; \ldots \; \left(\bm{\beta}_R^J\right)^T \right]^T$ and  $\bm{\beta}^K = \left[\left(\bm{\beta}_1^K\right)^T \; \ldots \; \left(\bm{\beta}_R^K\right)^T \right]^T$.
\vspace{5 pt}
\noindent In order to simplify the calculations, while ensuring that the penalty continues to promote sparse models, we adapt the definition of the lasso penalty. The new penalty defines the following optimization problem: %\in \mathbb{R}\times \mathbb{R}^{RJ} \times  \mathbb{R}^{RK} \times \mathbb{R}^M
\begin{equation}
 \beta_0, \bm{\beta}^J, \bm{\beta}^K, \bm{\beta}_{\text{uni}} \; = \hspace{-8pt} \underset{\beta_0 ,\, \bm{\beta}^J, \, \bm{\beta}^K, \, \bm{\beta}_{\text{uni}}}{\text{argmin}} \left( - \sum\limits_{i = 1}^{N} \log(\mathfrak{L}(y_i, \,\beta_0 ,\, \bm{\beta}^J, \, \bm{\beta}^K, \, \bm{\beta}_{\text{uni}})) + \sum\limits_{r = 1}^R 
    \lVert \bm{\beta}_r^K \otimes \bm{\beta}_r^J \rVert_1 + \lVert \bm{\beta}_{\text{uni}} \rVert_1 \right)
\end{equation}

\noindent Optimization is performed by alternating directions between $\left[ \beta_0 \; \; \left(\bm{\beta}^J \right)^T \; \; \left( \bm{\beta}_{\text{uni}} \right)^T \; \right]^T$ and  $\left[ \beta_0 \; \; \left(\bm{\beta}^K \right)^T \; \; \left( \bm{\beta}_{\text{uni}} \right)^T \; \right]^T$. The stopping criterion is defined by the relative difference between the value of the objective function after optimization in the first direction and the value of the same function after optimization in the second direction.  We note that optimizing the loss function in each of these directions is tantamount to performing a simple logistic regression with a lasso penalty. Indeed, if we denote $C$ the loss function of classical logistic regression penalized by lasso: 

\begin{equation}
%% Gerer l'allign
C: \begin{cases} 
    \begin{array}{ccl}
    \mathbb{R} \times \mathbb{R}^{K_0} \times \mathbb{R}^{N \times K_0} \times \mathbb{R}^{N} \times \mathbb{R} & \longrightarrow & \mathbb{R} \\
    (\beta_0, \bm{\beta}, \mathbf{X}, \mathbf{y}, \lambda ) & \longmapsto & -\displaystyle{\sum\limits_{i = 1}^N} [ y_i(\beta_0 + \mathbf{x}_i^T \bm{\beta}) - \log(1 + \exp(\beta_0 + \mathbf{x}_i^T \bm{\beta})) ] + \lambda \lVert \bm{\beta} \rVert_1
     \end{array}
\end{cases}
\end{equation}
($K_0 \in \mathbb{N}^{*}$)

\noindent optimizing the overall loss function with respect to $\left[ \beta_0 \; \; \left(\bm{\beta}^J \right)^T \; \; \left( \bm{\beta}_{\text{uni}} \right)^T \; \right]^T$ amounts to solve
\begin{equation}
\underset{(\beta_0, \bm{\beta}) \in \mathbb{R} \times \mathbb{R}^{JR + M}}{\text{argmin}} \left( C(\beta_0, (\mathbf{Q}^J)^{-1}\bm{\beta},\mathbf{Z}^J \mathbf{Q}^J, \mathbf{y}, \lambda) \right)
\end{equation}
\noindent Where $\mathbf{Q}^J$ and $\mathbf{Z}^J$ are defined as follows: 
\begin{align}
\mathbf{Z}^J &= [\overset{\raisebox{-1ex}{\scalebox{0.8}{$\bullet$}}}{\mathbf{Z}}\mathrlap{^J} \; \; \; \; \mathbf{X}_{\text{uni}}]\\
\text{with} \; \; \; \mathbf{Z}_{\text{tens}}^J	&= [\mathbf{Z}_1^J \; \; \hdots \; \; \mathbf{Z}_R^J]\\
\text{where} \; \forall r \in \llbracket 1, R\rrbracket\, , \hspace{0.5 cm} \mathbf{Z_r^J} &= \sum\limits_{k = 1}^K \mathbf{X}_{::k} (\beta_r^K)_k \hspace{0.5 cm} (\mathbf{Z}_r^J \in \mathbb{R}^{N \times J})\\
\hspace{7 pt}
\mathbf{Q}^J &= \text{Diag}([\mathbf{u}_{\text{tens}}^T \; \; \mathbbm{1}_M^T])\\
\text{with} \; \; \; \mathbf{u}_{\text{tens}} &= (\lVert \bm{\beta}_1^K \rVert_1^{-1} \mathbbm{1}_J^T, \; \; \hdots \; \; , \lVert \bm{\beta}_R^K \rVert_1^{-1} \mathbbm{1}_J^T)^T
\end{align}

 \noindent \textit{Girka et al.} (2021) \cite{multi_rank_r} demonstrate this result by noting that for $i \in \llbracket 1, n\rrbracket$,

 \begin{align}
      (\mathbf{x}_{\text{tens}})_i^T \left( \sum\limits_{r = 1}^R \bm{\beta}_r^K \otimes \bm{\beta}_r^J \right) &= \sum\limits_{r = 1}^R \left[(\mathbf{x}_{\text{tens}})_i^T   \left( \bm{\beta}_r^K  \otimes \mathbf{I}_J\right)\right] \bm{\beta}_r^J\\
      &= \sum\limits_{r = 1}^R (\mathbf{z}_{i,r}^J)^T \bm{\beta}_r^J\\
      &= (\mathbf{z}_{\text{tens}}^J)_i^T \bm{\beta}^J \hspace{0.5 cm} \text{On peut potentiellement dégager cette ligne}
    \end{align}

\noindent and that

\begin{align}
    &\sum\limits_{r = 1}^R \lVert \bm{\beta}_r^K \otimes \bm{\beta}_r^J \rVert_1 = \lVert \mathbf{R}_{\text{tens}}^J \bm{\beta}^J\rVert_1\\
    \text{with} \hspace{0.5 cm} &\mathbf{R}_{\text{tens}}^J = \text{Diag}(\lVert \bm{\beta}_1^K \rVert_1 \mathbbm{1}_J^T \; \; \hdots \; \;  \lVert \bm{\beta}_R^K \mathbbm{1}_J^T\rVert_1)
\end{align}

\noindent Thus,
\begin{align}
    &(\mathbf{x}_{\text{tot}})_i^T \bm{\beta}= (\mathbf{z}_i^J)^T \bm{\beta}^J  \\
    \text{and} \hspace{0.5 cm} & \sum\limits_{i = 1}^N 
    \lVert \bm{\beta}_r^K \otimes \bm{\beta}_r^J \rVert_1 + \lVert \bm{\beta}_{\text{uni}} \rVert_1= \lVert (\mathbf{Q}^J)^{-1} \bm{\beta} \rVert_1
\end{align}
This justifies the previous results\\[5 pt]
\noindent For optimization with respect to $\left[ \beta_0 \; \; \left(\bm{\beta}^K \right)^T \; \; \left( \bm{\beta}_{\text{uni}} \right)^T \; \right]^T$ , the method is analogous. The only difference concerns the definition of $\mathbf{Z}_{\text{tens}}^K$. It is:
\begin{align}
& \mathbf{Z}_{\text{tens}}^K = [\mathbf{Z}_1^K \; \hdots \; \mathbf{Z}_R^K]\\
\text{with} \forall r \in \llbracket 1, R \rrbracket \hspace{0.5 cm} & \mathbf{Z}_r^K = \sum\limits_{j = 1}^J (\beta_r^J)_j \mathbf{X}_{:j:}
\end{align}

\noindent This is justified by:

\begin{align}
    (\mathbf{x}_{\text{tens}})_i^T \left( \sum\limits_{r = 1}^R \bm{\beta}_r^K \otimes \bm{\beta}_r^J \right) &= \sum\limits_{r = 1}^R \left[ (\mathbf{x}_{\text{tens}})_i^T \left( I_K \otimes \bm{\beta}_r^J \right) \right] \bm{\beta}_r^K\\
    & = \sum\limits_{r = 1}^R (\mathbf{z}_{i,r}^K)^T \bm{\beta}_r^K\\
    & = (\mathbf{z}_{\text{tens}}^K)_i^T \bm{\beta}^K
\end{align}

\subsection{Multiway and multibloc logistic regression with lasso}

We now present the lasso-penalized multiway and multiblock logistic regression. This model draws heavily on the multiway logistic regression we have just presented, but adds a separation of the measured quantities into several categories. First-order quantities are placed in a first block, those concerning tumor shape in a second block and those concerning tumor texture in a third block. As clinical quantities are not measured at different times, they are not placed in a particular block. They will be included in the model in the same way as in the multiway case. Mathematically, we define the model as follows:\\
\indent Let $L \in \mathbb{N}^{*}$ denote the number of blocks of variables. For any $l \in \llbracket 1, L \rrbracket$, let $d_l$ be the number of radiomic quantities of interest in block $l$. Thus we have :
$$\sum\limits_{l = 1}^L d_l = J$$
We reorganize the columns of $\mathbf{X}_{\text{tot}}$ by grouping together columns associated with quantities from the same block. The resulting matrix is called $\mathbf{X}_{\text{tot}}^{'}$. More precisely, the order of the columns (from left to right) of the matrix $\mathbf{X}_{\text{tot}}^{'}$ is lexicographic according to the block number $l$ and then according to the observation time number $k$. This matrix is then multiplied by $\bm{\beta}$ in the equation that gives the likelihood \eqref{eqref:vraisemblance}. Finally, for $l \in \llbracket 1, L \rrbracket$, we call $\underline{\mathbf{X}}^l \in \mathbb{R}^{N \times d_l \times K}$ the tensor obtained by keeping only the radiomic quantities of the $l$-th block. Consistently with the notations used previously, we call $\mathbf{X}_{(1)}^l$ the matrix obtained by unfolding $\underline{\mathbf{X}}^l$, in which the individuals are arranged in rows and the order of the columns is lexicographic according to the value of $k$ and then $j$.
The new $\bm{\beta}$ structure is defined by blocks. It is:
\begin{equation}
\bm{\beta} = \left[ \left(\sum\limits_{r = 1}^R \bm{\beta}_{(1,r)}^K \otimes \bm{\beta}_{(1,r)}^J \right)^T \; \; \; \; \hdots \; \; \; \;  \left(\sum\limits_{r = 1}^R \bm{\beta}_{(L,r)}^K \otimes \bm{\beta}_{(L,r)}^J \right)^T \; \; \; \; \; \;\bm{\beta}_{\text{uni}}^T  \; \right]^T
\end{equation}
With for all $(l,r) \in \llbracket 1,L \rrbracket \times \llbracket 1, R\rrbracket$, $\bm{\beta}_{(l,r)}^J \in \mathbb{R}^{d_l}$ and $\bm{\beta}_{(l,r)}^K \in \mathbb{R}^{K}$  \\
In line with the notations of the multiway logistic model, we note:
\begin{equation}
    \bm{\beta}_{\text{tens}} = \left[ \left(\sum\limits_{r = 1}^R \bm{\beta}_{(1,r)}^K \otimes \bm{\beta}_{(1,r)}^J \right)^T \; \; \; \; \hdots \; \; \; \;  \left(\sum\limits_{r = 1}^R \bm{\beta}_{(L,r)}^K \otimes \bm{\beta}_{(L,r)}^J \right)^T \;  \right]
\end{equation}
We now call $\bm{\beta}^J$ and $\bm{\beta}^K$ the vectors
\begin{align}
    \bm{\beta}^J &= \left[ \left(\bm{\beta}_{(1,1)}^J \right)^T \hspace{7 pt} \hdots \hspace{7 pt} \left(\bm{\beta}_{(l = 1,r = R)}^J \right)^T  \hspace{7 pt}  \left(\bm{\beta}_{(l = 2,r = 1)}^J \right)^T  \hspace{7 pt} \hdots \hspace{7 pt}  \hdots \hspace{7 pt}  \left(\bm{\beta}_{(l = L,r = R)}^J \right)^T  \right]^T\\
    \bm{\beta}^K &= \left[ \left(\bm{\beta}_{(l = 1,r = 1)}^K \right)^T \hspace{7 pt} \hdots \hspace{7 pt} \left(\bm{\beta}_{(l = 1,r = R)}^K \right)^T  \hspace{7 pt}  \left(\bm{\beta}_{(l = 2,r = 1)}^K \right)^T  \hspace{7 pt} \hdots \hspace{7 pt}  \hdots \hspace{7 pt}  \left(\bm{\beta}_{(l = L,r = R)}^K \right)^T  \right]^T
\end{align}
% Indiquer qui est l et r après et changer les beta montrés (cf cahier)

\noindent In a similar way to what is done in the multiway model, we adapt the lasso penalty, so that the new optimization problem becomes:
\begin{equation}
    \beta_0 ,\, \bm{\beta}^J, \, \bm{\beta}^K, \, \bm{\beta}_{\text{uni}} = \hspace{- 8 pt} \underset{\beta_0 ,\, \bm{\beta}^J, \, \bm{\beta}^K, \, \bm{\beta}_{\text{uni}}}{\text{argmin}} \left( - \sum\limits_{i = 1}^{N} \log(\mathfrak{L}(y_i, \, \beta_0 ,\, \bm{\beta}^J, \, \bm{\beta}^K, \, \bm{\beta}_{\text{uni}})) + \sum\limits_{l = 1}^L \sum\limits_{r = 1}^R 
    \lVert \bm{\beta}_{(l,r)}^K \otimes \bm{\beta}_{(l,r)}^J \rVert_1 + \lVert \bm{\beta}_{\text{uni}} \rVert_1 \right)
\end{equation}

\noindent Once again, this problem is solved by alternating optimization directions $\left[ \beta_0 \; \; \left(\bm{\beta}^J \right)^T \; \; \left( \bm{\beta}_{\text{uni}} \right)^T \; \right]^T$ and\\
$\left[ \beta_0 \; \; \left(\bm{\beta}^K \right)^T \; \; \left( \bm{\beta}_{\text{uni}} \right)^T \; \right]^T$. Each of these two problems can be reduced to a lasso-penalized (matrix) logistic\\[3 pt]
 regression.\\
Indeed, optimizing according to $\left[ \beta_0 \; \; \left(\bm{\beta}^J \right)^T \; \; \left( \bm{\beta}_{\text{uni}} \right)^T \; \right]^T$ is equivalent to searching
\begin{equation}
\underset{(\beta_0, \bm{\beta}) \in \mathbb{R} \times \mathbb{R}^{RJ + M}}{\text{argmin}} \left( C(\beta_0, (\mathbf{Q}^J)^{-1}\bm{\beta},\mathbf{Z}^J \mathbf{Q}^J, \mathbf{y}, \lambda) \right)
\end{equation}
Where $\mathbf{Q}^J$ and $\mathbf{Z}^J$ are defined as follows:

\begin{align}
\mathbf{Z}^J &= [\mathbf{Z}_{\text{tens}}^J \; \; \mathbf{X}_{\text{uni}}] \label{eqref: Z_j}\\
\text{with} \; \; \; \mathbf{Z}_{\text{tens}}^J	&= \left[\mathbf{Z}_{(l= 1, r = 1)}^J \; \; \hdots \; \; \mathbf{Z}_{(l = 1, r = R)}^J  \; \; \hdots \; \; \mathbf{Z}_{(l = 2, r = 1)}^J \; \; \hdots \; \;  \hdots \; \; \mathbf{Z}_{(l = L, r = R)}^J\right] \label{eqref: Z_tens}\\
\text{where} \; \forall r \in \llbracket 1, R\rrbracket\, , \hspace{0.5 cm} \mathbf{Z_{(l,r)}^J} &= \sum\limits_{k = 1}^K (\mathbf{X})_{::k} (\beta_{(l,r)}^K)_k \hspace{0.5 cm} (\mathbf{Z_{(l,r)}^J} \in \mathbb{R}^{n \times d_l}) \label{eqref: Z_r}\\
\hspace{7 pt}
\mathbf{Q}^J &= \text{Diag}([\mathbf{u}_{\text{tens}}^T \; \; \mathbbm{1}_M^T]) \label{eqref:Q^J}\\
\text{with} \; \; \; \mathbf{u}_{\text{tens}} = (\lVert \bm{\beta}_{(l= 1, r = 1)}^K \rVert_1^{-1} \mathbbm{1}_{d_1}^T \; \; \hdots \; \; & \lVert \bm{\beta}_{(l = 1, r = R)}^K \rVert_1^{-1} \mathbbm{1}_{d_1}^T  \; \; \hdots \; \;  \lVert \bm{\beta}_{(l = 2, r = 1)}^K \rVert_1^{-1} \mathbbm{1}_{d_2}^T     \; \; \hdots \; \;  \hdots \; \; \lVert \bm{\beta}_{(l = L, r = R)}^K \rVert_1^{-1} \mathbbm{1}_{d_L}^T  )^T \label{eqref:vec_q}
\end{align}

The demonstration of this result is similar to that of the multiway case. Indeed, we note that
\begin{align}
    (\mathbf{x}_{\text{tens}}^{'})_i^T \bm{\beta}_{\text{tens}} &= \sum\limits_{l = 1}^L \sum\limits_{r = 1}^R (\mathbf{x}_{(1)}^l)_i^T \left(\bm{\beta}_{(l,r)}^K \otimes \bm{\beta}_{(l,r)}^J \right)\\
    &= \sum\limits_{l = 1}^L \sum\limits_{r = 1}^R \left[ (\mathbf{x}_{(1)}^l)_i^T \left( \bm{\beta}_{(l,r)}^K \otimes I_{d_l} \right) \right]\bm{\beta}_{(l,r)}^J\\ 
    &= \sum\limits_{l = 1}^L \sum\limits_{r = 1}^R (z_{(l,r)}^J)_i^T \bm{\beta}_{(l,r)}^J\\
    &= (z_{\text{tens}}^J)_i^T \bm{\beta}^J
\end{align}

An that

\begin{align}
    \sum\limits_{l = 1}^L \sum\limits_{r = 1}^R \lVert \bm{\beta}_{(l,r)}^K \otimes \bm{\beta}_{(l,r)}^J \rVert_1 &= \lVert \mathbf{R}_{\text{tens}}^J \bm{\beta}^J\rVert_1\\
    \text{with} \hspace{0.5 cm} \mathbf{R}_{\text{tens}}^J = \text{Diag}    (\lVert \bm{\beta}_{(l= 1, r = 1)}^K \rVert_1 \mathbbm{1}_{d_1}^T \; \; \hdots \; \; & \lVert \bm{\beta}_{(l = 1, r = R)}^K \rVert_1 \mathbbm{1}_{d_1}^T  \; \; \hdots \; \;  \lVert \bm{\beta}_{(l = 2, r = 1)}^K \rVert_1 \mathbbm{1}_{d_2}^T     \; \; \hdots \; \;  \hdots \; \; \lVert \bm{\beta}_{(l = L, r = R)}^K \rVert_1 \mathbbm{1}_{d_L}^T  )^T
\end{align}

\noindent We deduce that

\begin{align}
    &(\mathbf{x}_{\text{(1)}}^{'})_i^T \bm{\beta}= (\mathbf{z}_i^J)^T \bm{\beta}^J  \\
    \text{and} \hspace{0.5 cm} & \sum\limits_{l = 1}^L \sum\limits_{r = 1}^R \lVert \bm{\beta}_{(l,r)}^K \otimes \bm{\beta}_{(l,r)}^J \rVert_1 + \lVert \bm{\beta}_{\text{uni}} \rVert_1= \lVert (\mathbf{Q}^J)^{-1} \bm{\beta} \rVert_1
\end{align}

\noindent Wich justifies the previous results.\\[5 pt]
\noindent For optimization with respect to $\left[ \beta_0 \; \; \left(\bm{\beta}^K \right)^T \; \; \left( \bm{\beta}_{\text{uni}} \right)^T \; \right]^T$ , the method is analogous. The only difference concerns the form of $\mathbf{Z}_{\text{tens}}^K$. It is written as:
\begin{align}
    \mathbf{Z}_{\text{tens}}^K	&= \left[\mathbf{Z}_{(l= 1, r = 1)}^K \; \; \hdots \; \; \mathbf{Z}_{(l = 1, r = R)}^K  \; \; \hdots \; \; \mathbf{Z}_{(l = 2, r = 1)}^K \; \; \hdots  \; \;\hdots \; \; \mathbf{Z}_{(l = L, r = R)}^K \right] \label{eqref: Z^K}\\
    \text{where} \; \forall r \in \llbracket 1, R\rrbracket\, , \hspace{0.5 cm} \mathbf{Z_{(l,r)}^K} &= \sum\limits_{j = 1}^{d_l} (\mathbf{X})_{:j:} (\beta_{(l,r)}^J)_j \hspace{0.5 cm} (\mathbf{Z_{(l,r)}^K} \in \mathbb{R}^{N \times K}) \label{eqref: Z_tens^K}\\
\end{align}

\noindent The justification of that last result is analogous to the one used in the multiway case.\\[2 pt]

\noindent \textbf{Notes}: 
\begin{itemize}
\item In the multiway and multibloc logistic regression with lasso, it is possible to allow for the choice of a specific rank for each block. We have not yet studied this model extension, but deriving its equations is straightforward, based on the equations provided in this report.
\item We decided to optimize the loss function completely in one direction before turning to the other one instead of alternating one step in each direction because the first procedure was more stable and could be implemented efficiently using the glmnet package in R \cite{glmnet}. 
\end{itemize}

\vspace{7 pt}

{\fontsize{12}{8}\selectfont \noindent \textbf{Pseudo-code:}}\\[1 pt] 
In order to clarify the algorithm that we use, we give here the pseudo-code of our implementation. In order to be more readable, we keep the notations that were used during the presentation of the model.\\[5 pt]
\newpage
\begin{mdframed}[leftmargin=0cm, rightmargin=4cm]
\noindent \textbf{Inputs}\\
\phantom{a}\hspace{5 pt} $\bullet$ $\epsilon >0$, $\lambda >0$, $R \in \mathbb{N}^{*}$\\[2 pt]
\phantom{a}\hspace{5 pt} $\bullet$ $\bm{\beta}^{K(0)} \in \mathbb{R}^{LRK}$\\[4 pt]
\textbf{Treatment}\\
\phantom{a}\hspace{5 pt} $\bullet$ $q \leftarrow 0$\\[2 pt]
\phantom{a}\hspace{5 pt}  \textbf{Repeat}\\[2 pt]
\begin{tikzpicture}[overlay, remember picture]
    \draw[thick] (0.9, 0.18) -- (0.9, -3.75);  % Modifier les coordonnées pour ajuster la position et la
\end{tikzpicture}
\phantom{a}\hspace{22 pt} $\bullet$ Construct $\mathbf{Z}^J$ according to \cref{eqref: Z_j,eqref: Z_r,eqref: Z_tens}\\[2 pt]
\phantom{a}\hspace{25 pt} $\bullet$ Construct $\mathbf{Q}^J$ according to \cref{eqref:Q^J,eqref:vec_q}\\[2 pt]
\phantom{a}\hspace{25 pt}  $\bullet$ $(\beta_0^{(q)}, \bm{\beta}^{J {(q)}}) \longleftarrow \hspace{-10 pt} \underset{(\beta_0, \bm{\beta}) \in \mathbb{R} \times \mathbb{R}^{RJ + M}}{\text{argmin}} \left( C(\beta_0, (\mathbf{Q}^J)^{-1}\bm{\beta},\mathbf{Z}^J \mathbf{Q}^J, \mathbf{y}, \lambda) \right)$\\[2 pt]
\phantom{a}\hspace{25 pt} $\bullet$ Construct $\mathbf{Z}^K$ according to \cref{eqref: Z^K,eqref: Z_tens^K}\\[3 pt]
\phantom{a}\hspace{25 pt} $\bullet$ Construct $\mathbf{Q}^K$ by adapting \cref{eqref:Q^J,eqref:vec_q}\\[2 pt]
\phantom{a}\hspace{25 pt}  $\bullet$ $(\beta_0^{(q)}, \bm{\beta}^{K {(q)}}) \longleftarrow \hspace{-10 pt} \underset{(\beta_0, \bm{\beta}) \in \mathbb{R} \times \mathbb{R}^{LRK + M}}{\text{argmin}} \left( C(\beta_0, (\mathbf{Q}^K)^{-1}\bm{\beta},\mathbf{Z}^K \mathbf{Q}^K, \mathbf{y}, \lambda) \right)$\\[2 pt]
\phantom{a}\hspace{25 pt}  $\bullet$ $q \leftarrow q + 1$\\[4 pt]
\phantom{a}\hspace{8 pt}  \textbf{until} $|C^K - C^J| < \epsilon |C^J| $\\[4 pt]
\textbf{Return} $(\beta_0^{(q)},\bm{\beta}^{K(q)}, \bm{\beta}^{J(q)})$
\end{mdframed}





\vspace{10 pt}













\vspace{2 cm}
 
\section{results}




\section*{Conclusions}
\blindtext




\bibliographystyle{ieeetr} %alpha, apalike, ieeetr
\bibliography{bibliography.bib}

%\begin{thebibliography}{99} % Bibliography - this is %intentionally simple in this template

%\bibitem{andreotti1997studying} B. Andreotti, \emph{Studying Burgers' models to investigate the physical meaning of the alignments statistically observed in turbulence}, Phys. Fluids \textbf{9} : 3, March (1997)

%\bibitem{cohl1999compact} Cohl, Howard S., and Joel E. Tohline, \emph{A compact cylindrical Green's function expansion for the solution of potential problems}, The astrophysical journal \textbf{527} : 86 - 101 (1999) %DOI: https://doi.org/10.1086/308062

%\bibitem{abramowitz1965handbook} Abramowitz, Milton, and Irene A. Stegun. \emph{Handbook of Mathematical Functions With Formulas, Graphs, and Mathematical Tables.} (1964).


%\end{thebibliography}

%croos section karlie
%GOOD: http://www.eumetrain.org/satmanu/CMs/TrCyAt/print.htm 
%https://physics.stackexchange.com/questions/275799/why-is-the-eye-of-a-cyclone-a-forced-vortex
%http://www.chanthaburi.buu.ac.th/~wirote/met/tropical/textbook_2nd_edition/navmenu.php_tab_9_page_7.1.0.htm
%http://www.atmos.umd.edu/~dalin/andrew/part2.html
%https://nptel.ac.in/courses/119102007/2
%http://www.911omissionreport.com/steering_hurricanes.html
%https://www.youtube.com/watch?v=_brY_9ME8iE brooks
\end{document}